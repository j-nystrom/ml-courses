{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c19073",
   "metadata": {},
   "source": [
    "# Lab: Image classification with deep learning\n",
    "\n",
    "#### Course: Statistical machine learning\n",
    "\n",
    "Authors: Jakob Nyström, Inga Wohlert (group 19)\n",
    "\n",
    "## 3. Preparation exercises\n",
    "\n",
    "### 3.1 Softmax and cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fab42",
   "metadata": {},
   "source": [
    "#### Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f3139c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62c86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return np.exp(x) / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8beb430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=1: 0.731\n",
      "p=-1: 0.269\n"
     ]
    }
   ],
   "source": [
    "z = 1\n",
    "h_z = sigmoid(z)\n",
    "p_1 = h_z\n",
    "p_neg_1 = 1 - h_z\n",
    "\n",
    "print(f\"p=1: {round(p_1, 3)}\")\n",
    "print(f\"p=-1: {round(p_neg_1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466aa14",
   "metadata": {},
   "source": [
    "**Answer:** $p(y = 1 \\, | \\, x) \\approx 0.731$ and $p(y = 1 \\, | \\, x) \\approx 0.269$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54c0e3",
   "metadata": {},
   "source": [
    "#### Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f85d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z, z_list):\n",
    "    \n",
    "    return np.exp(z) / np.sum(np.exp(z_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70d15958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=1: 0.245\n",
      "p=2: 0.09\n",
      "p=3: 0.665\n"
     ]
    }
   ],
   "source": [
    "z_list = [0, -1 , 1]\n",
    "probas = []\n",
    "\n",
    "for z in z_list:\n",
    "    res = softmax(z, z_list)\n",
    "    probas.append(res)\n",
    "    \n",
    "for count, prob in enumerate(probas, start=1):\n",
    "    print(f\"p={count}: {round(prob,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1af08c",
   "metadata": {},
   "source": [
    "**Answer:** Class y = 3 has the highest probability, $0.665$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fa73a",
   "metadata": {},
   "source": [
    "#### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4cf48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_vec, class_proba):\n",
    "    \n",
    "    y_vec = np.array(y_vec)\n",
    "    class_proba = np.array(np.log(class_proba))\n",
    "    cross_entropy = -np.dot(y_vec, class_proba)\n",
    "    \n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5c986de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=1: 1.408\n",
      "p=2: 2.408\n",
      "p=3: 0.408\n"
     ]
    }
   ],
   "source": [
    "y_1 = [1, 0, 0]\n",
    "y_2 = [0, 1, 0]\n",
    "y_3 = [0, 0, 1]\n",
    "y_list = [y_1, y_2, y_3]\n",
    "\n",
    "class_proba = probas\n",
    "cross_entropies = []\n",
    "\n",
    "for y in y_list:\n",
    "    res = cross_entropy(y, class_proba)\n",
    "    cross_entropies.append(res)\n",
    "    \n",
    "for count, ce in enumerate(cross_entropies, start=1):\n",
    "    print(f\"p={count}: {round(ce, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a7ce3",
   "metadata": {},
   "source": [
    "**Answer:** y = 3 has the lowest cross-entropy, which makes sense since it has the highest probability.\n",
    "\n",
    "### 3.2 Dense neural network\n",
    "\n",
    "#### Question 3.4\n",
    "\n",
    "Sizes of the weight matrices and offset vectors\n",
    "\n",
    "- $W^{(1)}$: 30 x 144\n",
    "- $b^{(1)}$: 30 x 1\n",
    "- $W^{(2)}$: 4 x 30 \n",
    "- $b^{(2)}$: 4 x 1\n",
    "\n",
    "Number of parameters in the network is 4,474 (see calculation below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11505543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n"
     ]
    }
   ],
   "source": [
    "params = (30 * 144 + 30) + (4 * 30 + 4)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e125b8",
   "metadata": {},
   "source": [
    "### 3.3 Convolutional neural network\n",
    "\n",
    "#### Question 3.5\n",
    "\n",
    "$W$ has dimensions filter rows x filter columns x input channels x output channels; $b$ has dimension equal to the number of filters; and $Q$: $(k \\, / \\, s_k)$ x $(l \\, / \\,s_l)$ x output channels, where $k$ is the width of the image, $l$ is the height of the image and $s_k, s_l$ are the stride parameters. \n",
    "\n",
    "- $W^{(1)}$: 5 x 5 x 1 x 4 \n",
    "- $b^{(1)}$: 1 x 4  \n",
    "- $Q^{(1)}$: 12 x 12 x 4   \n",
    "\n",
    "\n",
    "#### Question 3.6\n",
    "\n",
    "- $W^{(2)}$: 3 x 3 x 4 x 8 \n",
    "- $b^{(2)}$: 1 x 8  \n",
    "- $Q^{(2)}$: 6 x 6 x 8\n",
    "\n",
    "#### Question 3.7\n",
    "\n",
    "- $W^{(3)}$: 60 x 288\n",
    "- $b^{(3)}$: 60 x 1 \n",
    "- $W^{(4)}$: 4 x 60 \n",
    "- $b^{(4)}$: 4 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23979056",
   "metadata": {},
   "source": [
    "## 4. Lab exercises\n",
    "\n",
    "### Classification of handwritten digits\n",
    "\n",
    "#### Question 4.1: Single layer network (essentially multi-class logistic regression)\n",
    "\n",
    "Test accuracy is 92.29%.\n",
    "\n",
    "#### Question 4.2: Number of params in the single-layer network\n",
    "\n",
    "$\\mathbf{W}$ contains $784 x 10 = 7,840$ parameters; $\\mathbf{b}$ contains $10$ (that are the same for all $n$ rows). So in total there are $7,850$ parameters.\n",
    "\n",
    "#### Question 4.3: Number of batches and epochs in training \n",
    "\n",
    "There are $n = 60,000$ training observations, so for a mini-batch size of $100$ that means we have $60,000 / 100 = 600$ iterations in each epoch. There are $2,000$ epochs during training. \n",
    "\n",
    "#### Question 4.4: Performance when adding one hidden layer\n",
    "\n",
    "With a hidden layer of 200 units we get the following test performance: 94.58%\n",
    "\n",
    "Varying the number of units in the hidden layer, ranging between 10 to 750, give these results\n",
    "\n",
    "- 10 units: 91.6%\n",
    "- 25 units: 93.76%\n",
    "- 50 units: 94.5%\n",
    "- 100 units: 94.76%\n",
    "- 200 units: 94.58%\n",
    "- 300 units: 94.75%\n",
    "- 500 units: 94.69%\n",
    "- 750 units: 94.89%\n",
    "\n",
    "A certain number (at least bigger than 10 or 25) is required to capture the complexity in the data, but at some point around 100 units, adding more does not significantly improve performance of the model. \n",
    "\n",
    "Using $U = 200$ but initializing the weight vectors to $0$ instead of randomly means significantly lower test accuracy of 72.47% as the model easier gets stuck in a suboptimal local minima or other stationary point (saddle point).\n",
    "\n",
    "#### Question 4.5: Performance when adding several more hidden layers\n",
    "\n",
    "Classification accuracy on the test set is now only 11.35%. Since all weight vectors are initialized to zero, we are likely to get a lot of zero gradients, which means we will get stuck in some stationary point very quickly.\n",
    "\n",
    "#### Task 4.6: Tuning the multi-layer model\n",
    "\n",
    "Test accuracy improves significantly, up to 97.22%, when randomly initializing weights, changing the initialization of the offset vectors and swapping the solver.\n",
    "\n",
    "#### Task 4.7: Increasing the number of iterations\n",
    "\n",
    "There is still some oscillation in the test accuracy for the last 500 or so iterations, which can indicate that there is still some potential by increasing the number of epochs. \n",
    "\n",
    "Changing to 10,000 iterations yields 97.93% accuracy and seemingly no issues with numerical instability, when using SGD. However, when switching to the Adam solver, the algorithm breaks down after 5,300 iterations and test accuracy drop from 97.82% to 9.8%.\n",
    "\n",
    "#### Task 4.8 / Question 4.6: Dealing with numerical instability\n",
    "\n",
    "When changing the cross entropy function, we don't have instability, and manage to get 98.09% test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e14cf9",
   "metadata": {},
   "source": [
    "### Use convolutional neural networks\n",
    "\n",
    "#### Question 4.7\n",
    "\n",
    "There are a total of $7 x 7 x 12 = 588$ hidden units in the third convolutional layer, with 49 in each channel. \n",
    "\n",
    "#### Question 4.8\n",
    "\n",
    "For the convolutional neural net we get 98.55% accuracy on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21305b97",
   "metadata": {},
   "source": [
    "### Real world image classification\n",
    "\n",
    "#### Task 4.14\n",
    "\n",
    "The model does very well on the animal and object pictures. However, it incorrectly classified the hedgehog as a porcupine. When dealing with more \"abstract\" entities like the Pelle Svanslös statue, it goes wrong. The same when testing pictures of humans - apparently it's not trained on pictures of humans (rather, it identifies clothing in the image)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
